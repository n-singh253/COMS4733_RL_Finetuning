# Quick test configuration for PPO (5-10 minute test run)
# Use this to verify the pipeline works before running full training

policy:
  # Checkpoint from BC pre-training (warmup phase)
  checkpoint: ./runs/dinov2_bc_best.pt

  # PPO hyperparameters (tuned for BC warmstart with dense demos)
  learning_rate: 1e-7  # Ultra-low LR during value head warmup phase
  clip_range: 0.05  # Very tight clipping during warmup (will increase later)
  value_coef: 1.0  # Higher weight on value learning to fix random initialization
  entropy_coef: 0.01  # Moderate entropy to maintain exploration
  max_grad_norm: 0.5  # Gradient clipping
  action_std: 0.15  # Higher exploration noise to smooth out KL
  target_kl: 0.1  # More lenient during warmup phase

  # Training schedule - REDUCED FOR QUICK TEST
  num_epochs: 3  # Only 3 epochs for quick test
  rollout_length: 256  # Shorter rollouts (8x faster)
  ppo_epochs: 4  # More epochs to let value head learn (was 2)
  batch_size: 64  # Minibatch size for PPO updates

  # Discount and GAE
  gamma: 0.99  # Discount factor
  gae_lambda: 0.95  # GAE lambda parameter

  # Device (auto-detects cuda/mps/cpu)
  device: auto
  seed: 42

environment:
  # Environment configuration
  asset_root: ./env/mujoco_assets
  reward_type: shaped  # Use shaped rewards for better learning signal

logging:
  output_dir: ./runs/ppo_test
  # Checkpoints: Only saves best (by success rate) and last epoch automatically
