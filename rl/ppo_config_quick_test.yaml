# Quick test configuration for PPO (5-10 minute test run)
# Use this to verify the pipeline works before running full training

policy:
  # Checkpoint from BC pre-training (warmup phase)
  checkpoint: ./runs/dinov2_bc_best.pt

  # PPO hyperparameters (tuned for BC warmstart with dense demos)
  learning_rate: 1e-6  # Very low LR to prevent policy collapse (was 3e-6, caused KL=5.9)
  clip_range: 0.1  # Tighter clipping to stay close to BC initialization (was 0.2)
  value_coef: 0.5  # Coefficient for value loss
  entropy_coef: 0.0  # No entropy bonus - BC policy already has good exploration (was 0.01)
  max_grad_norm: 0.5  # Gradient clipping
  action_std: 0.05  # Smaller noise for fine-tuning (was 0.1)
  target_kl: 0.02  # Slightly higher tolerance (was 0.01, too strict for warmstart)

  # Training schedule - REDUCED FOR QUICK TEST
  num_epochs: 3  # Only 3 epochs for quick test
  rollout_length: 256  # Shorter rollouts (8x faster)
  ppo_epochs: 2  # Fewer update epochs
  batch_size: 64  # Minibatch size for PPO updates

  # Discount and GAE
  gamma: 0.99  # Discount factor
  gae_lambda: 0.95  # GAE lambda parameter

  # Device (auto-detects cuda/mps/cpu)
  device: auto
  seed: 42

environment:
  # Environment configuration
  asset_root: ./env/mujoco_assets
  reward_type: shaped  # Use shaped rewards for better learning signal

logging:
  output_dir: ./runs/ppo_test
  # Checkpoints: Only saves best (by success rate) and last epoch automatically
