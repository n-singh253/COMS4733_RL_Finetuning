# Quick test configuration for PPO (5-10 minute test run)
# Use this to verify the pipeline works before running full training

policy:
  # Checkpoint from BC pre-training (warmup phase)
  checkpoint: ./runs/dinov2_bc_best.pt

  # PPO hyperparameters
  learning_rate: 3e-6  # Lower LR for fine-tuning pre-trained model
  clip_range: 0.2  # Standard PPO clip range
  value_coef: 0.5  # Coefficient for value loss
  entropy_coef: 0.01  # Entropy bonus for exploration
  max_grad_norm: 0.5  # Gradient clipping
  action_std: 0.1  # Standard deviation for Gaussian policy
  target_kl: 0.01  # Target KL for early stopping

  # Training schedule - REDUCED FOR QUICK TEST
  num_epochs: 3  # Only 3 epochs for quick test
  rollout_length: 256  # Shorter rollouts (8x faster)
  ppo_epochs: 2  # Fewer update epochs
  batch_size: 64  # Minibatch size for PPO updates

  # Discount and GAE
  gamma: 0.99  # Discount factor
  gae_lambda: 0.95  # GAE lambda parameter

  # Device
  device: cuda
  seed: 42

environment:
  # Environment configuration
  asset_root: ./env/mujoco_assets
  reward_type: shaped  # Use shaped rewards for better learning signal

logging:
  output_dir: ./runs/ppo_test
  checkpoint_interval: 1  # Save every epoch for testing
  eval_interval: 1  # Evaluate every epoch
