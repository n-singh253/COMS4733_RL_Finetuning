# PPO configuration for RL fine-tuning of VLA policy
# Based on RL4VLA approach and PPO best practices

policy:
  # Checkpoint from BC pre-training (warmup phase)
  checkpoint: ./runs/dinov2_bc_best.pt

  # PPO hyperparameters (MODERATE - BC can grasp, just wrong transport direction)
  learning_rate: 1e-6  # Low but reasonable (BC is partially working!)
  clip_range: 0.05  # Moderate clipping
  value_coef: 0.5  # Coefficient for value loss
  entropy_coef: 0.0  # No entropy bonus
  max_grad_norm: 0.3  # Allow larger gradients
  action_std: 0.02  # Some exploration to find correct direction
  target_kl: 0.02  # Standard tolerance

  # Training schedule (constrained by user: max 35 epochs, 1024 rollout)
  num_epochs: 35  # Total number of rollout epochs
  rollout_length: 512  # Steps to collect per epoch (~5 complete episodes at 200 steps each)
  ppo_epochs: 2  # Two gradient steps (BC is good starting point)
  batch_size: 64  # Moderate batch size

  # Discount and GAE
  gamma: 0.99  # Discount factor
  gae_lambda: 0.95  # GAE lambda parameter

  # Device (auto-detects cuda/mps/cpu)
  device: auto
  seed: 42

environment:
  # Environment configuration
  asset_root: ./env/mujoco_assets
  reward_type: shaped  # "dense", "sparse", or "shaped" - shaped gives best learning signal

logging:
  output_dir: ./runs/ppo
  # Checkpoints: Only saves best (by success rate) and last epoch automatically
